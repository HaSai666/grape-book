# 5 图卷积神经网络

现有图神经网络皆基于邻居聚合的框架，即为每个目标节点通过聚合其邻居刻画结构信息，进而学习目标节点的表示。因此，在建模图神经网络时，研究 人员的关注重点是如何在网络上构建聚合算子，聚合算子的目的是刻画节点的局部结构。现有的聚合算子构建 分为谱方法和空间方法两类，谱方法利用图上卷积定理从谱域定义图卷积，而卷积算子本身即为聚合算子；而空间方法从节点域出发，通过在节点层面定义聚合函数来聚合每个中心节点和其邻近节点。


## 5.1 谱方法的背景知识

过去几年，卷积神经网络在图像领域大放异彩。卷积算子定义了加权和操作，其本身是聚合算子。借助于卷积神经网络对局部结构的建模能力以及网络 数据上普遍存在的节点依赖关系，通过定义网络数据上的卷积算子进而设计图 神经网络已经成为其中最活跃最重要的一支。但网络数据上平移不变性的缺失， 给在节点域定义卷积算子带来困难。谱方法利用卷积定理从谱域定义卷积算子。 我们首先给出卷积定理的背景知识。

### 图信号处理

卷积定理：信号卷积的傅立叶变换等价于信号傅立叶变换的乘积

$$
F(f*g) = F(f)\cdot F(g)  \tag{1}
$$

其中的 $f, g$ 表示两个原始信号，$F(f)$ 表示 $f$ 的傅立叶变换， $\cdot$ 表示乘积算子， $*$ 表示卷积算子。对上面的公式做傅立叶逆变换，可以得到

$$
f*g = F^{-1}(F(f)\cdot F(g)) \tag{2}
$$

其中 $F^{-1}(f)$ 表示信号 $f$ 的傅立叶逆变换。

利用卷积定理，我们可以对谱空间的信号做乘法，再利用傅里叶逆变换将信
号转换到原空间来实现图卷积，从而避免了图数据不满足平移不变性而造成的 卷积定义困难问题。图上傅立叶变换依赖于图上的拉普拉斯矩阵，下面，我们给出图上傅立叶变换的定义。图上傅立叶变换的定义依赖于拉普拉斯矩阵的特征向量。以特征向量作为谱空间下的一组基底，图上信号 $x$ 的傅立叶变换为：

$$
\hat{x} = U^Tx \tag{3}
$$

其中 $x$ 指信号在节点域的原始表示。$\hat{x}$ 指信号 $x$ 变换到谱域后的表示，$U^T$ 表示特征向量矩阵的转置，用于做傅立叶变换。信号 $x$ 的傅立叶逆变换为

$$
x = U\hat{x} \tag{4}
$$

利用图上傅立叶变换和逆变换，我们可以基于卷积定理实现图卷积算子：

$$
x *_{G} y = U((U^Tx)\odot(U^Ty))
$$

其中 $*_G$ 表示图卷积算子，$x,y$ 表示图上节点域的信号，$\odot$ 表示哈达玛积，表示 两个向量的对应元素相乘。我们用一个对角阵 $g_{\theta}$ 代替 $U^Ty$，那么哈达玛积可以转化成矩阵乘法。将卷积核 $g_{\theta}$ 作用在信号上，图卷积可以表示成 $Ug_{\theta}U^Tx$。

卷积定理提供了通过傅立叶变换在图上定义卷积的方式。基于此卷积算子的定义，国内外陆续涌现出一些基于卷积聚合的图神经网络。

### 基于卷积定理的图神经网络

谱卷积神经网络(Spectral CNN)是最早提出在网络数据上构建图神经网络的方法，该方法利用卷积定理在每一层定义图卷积算子，在损失函数指导下通过梯度反向回传学习卷积核，并堆叠多层组成神经网络。谱卷积神 经网络第 $m$ 层的结构如下

$$
X_j^{m+1} = h(U \sum_{i=1}^p F^m_{i,j}U^TX_i^m ) \quad j = 1, \cdots, q
$$

其中 $p, q$ 分别是输入特征和输出特征的维度，$X_i^m \in R^n$ 表示图上节点在第 $m$ 层的第 $i$ 个输入特征，$F_{i,j}^m$ 表示谱空间下卷积核，$h$ 表示非线性激活函数。在谱卷积神经网络中，这样一层结构将特征从 $p$ 维转化到 $q$ 维，且基于卷积定理通过学习卷积核实现了图卷积。

谱卷积神经网络将卷积核作用在谱空间的输入信号上，并利用卷积定理实现图卷积，以完成节点之间的信息聚合，然后将非线性激活函数作用在聚合结果上，并堆叠多层形成神经网络。但该模型不满足局部性，使得谱卷积神经网络的局部性没有保证，即产生信息聚合的节点并不一定是邻近节点。

建模图神经网络的初衷是为了利用图结构刻画邻近节点的信息聚合，而上面提到的谱卷积神经网络并不满足局部性，Henaff等提出用带有平滑性约束的插值卷积核，这种方法降低参数个数且实现了图神经网络的局部化。此外，还有一些工作致力于实现图神经网络的局部性和加速计算。这些工作通过参数化卷积核实现局部性，同时降低参数复杂度和计算复杂度。

$g_{\theta}$ 是需要学的卷积核，在谱卷积神经网络中，$g_{\theta}$ 对角阵的形式，且有 $n$ 个需要学的参数。切比雪夫网络(ChebyNet)对卷积核 $g_{\theta}$ 进行参数化

$$
g_{\theta}=\sum_{i=0}^{K-1} \theta_{k} T_{k}(\hat{\Lambda})
$$

其中 $\theta_{k}$ 是需要学的系数，$\hat{\Lambda}=\frac{2 \Lambda}{\lambda_{\max }}-I_{n}$。切比雪夫多项式是通过递归得到，递归表达式为

$$
T_{k}(x)=2 x T_{k-1}(x)-T_{k-2}(x)
$$

其中初始值 $T_0(x) = 1, T_1(x) = x$。

令 $\hat{L}=\frac{2 L}{\lambda_{\max }}-I_{n}$，切比雪夫网络第 $m$ 层的结构定义如下：

$$
\begin{aligned}
X_{j}^{m+1} &=h\left(U \sum_{i=1}^{p}\left(\sum_{k=0}^{K-1} \theta_{k} T_{k}(\hat{\Lambda})\right) U^{\top} X_{i}^{m}\right) \\
&=h\left(\sum_{i=1}^{p} \sum_{k=0}^{K-1} \theta_{k} T_{k}(\hat{L}) X_{i}^{m}\right) \qquad j=1, \cdots, q .
\end{aligned}
$$

切比雪夫网络利用特征值矩阵的多项式参数化卷积核，实现谱卷积神经网络，且巧妙的利用 $L = U \Lambda U^T$ 引入拉普拉斯矩阵，从而避免了拉普拉斯矩阵的特征分解，同时参数复杂度从 $O(n \times p \times q)$ 下降到 $O(K \times p \times q)$。此外，在拉普拉斯矩阵中，当且仅当节点 $i,j$ 满足 $K$ 跳可达时，$L^K_{i,j}\neq 0$，这一性质使得当 $K$ 较小时，切比雪夫网络具有局部性。

基于个性化 PageRank 的图卷积神经网络 (PPNP)和简明一阶图卷积神经网络 (SGC)则是对一阶图卷积神经网络方法进行 分析，并提出一些简化和变体。PPNP 从深层网络的搭建出发，指出随着模型层 数加深，网络拟合能力增强，但是在一阶图卷积神经网络中会引起节点表达过于平滑，进而导致节点不可区分的问题。基于此，PPNP 解耦维度变换和特征传播， 并引入个性化 PageRank，对输入数据先完成较少层数的维度变换，然后基于个性化 PageaRank 进行特征传播，特征传播过程不进行参数学习，因此可以用在半监督学习任务中。

简明一阶图卷积神经网络 (SGC) 指出，非线性变换在一阶图卷积神经网络中无足轻重，使得 GCN 发挥作用的是每一层的特征传播机制。基于此，SGC 抛弃层之间的非线性变换，将多个层的特征传播融合到一个层内，在完成特征传播后，SGC 对样本做一次维度变换。此模型等价于不含非线性变换的多层一阶图卷积神经网络。

切比雪夫网络和一阶图卷积神经网络着眼于参数化卷积核，以上方法虽然是从谱空间出发，但其最终形式已经包含定义节点相关性的聚合函数，从空间方法角度看，切比雪夫网络以拉普拉斯矩阵多项式作为聚合函数，一阶卷积神经网络以 $\hat{D}^{-1/2}\hat{A}\hat{D}^{-1/2}$ 作为聚合函数，其输出结果表示每个节点在该聚合函数下由自身和邻近节点加权得到的新表达。而基于个性化 PageRank 的图卷积神经网络 (PPNP) 和简明一阶图卷积神经网络 (SGC) 虽然是从一阶图卷积神经网络出发，但其已经是通过聚合函数分析节点特征传播。以上方法可以看作是谱方法和空间方法的桥梁。


## 5.2 GraphSAGE

GraphSAGE 是这篇论文提出来的 “Hamilton, Will, Zhitao Ying, and Jure Leskovec. "Inductive representation learning on large graphs." Advances in neural information processing systems. 2017.” 的一个图神经网络算法，是一个经典的基于空域的算法，它从两个方面对传统的GCN做了改进：（1）在训练时的，采样方式将GCN的全图采样优化到部分以节点为中心的邻居抽样，这使得大规模图数据的分布式训练成为可能，并且使得网络可以学习没有见过的节点，这也使得GraphSAGE可以做Inductive Learning。（2）GraphSAGE研究了若干种邻居聚合的方式，并通过实验和理论分析对比了不同聚合方式的优缺点。下面我们详细介绍算法细节。

在GraphSAGE之前的GCN模型中，都是采用的全图的训练方式，也就是说每一轮的迭代都要对全图的节点进行更新，当图的规模很大时，这种训练方式无疑是很耗时甚至无法更新的。mini-batch的训练时深度学习一个非常重要的特点，那么能否将mini-batch的思想用到GraphSAGE中呢，GraphSAGE提出了一个解决方案。它的流程大致分为3步：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="figures/5_graphsage_visual.png" width=400> 
    <br>
    <div style="color:orange;
    display: inline-block;
    color: #999;
    padding: 2px;">图5-1. GraphSAGE算法流程（包含采样和聚合）</div>
</center>

- 对邻居进行随机采样，每一跳抽样的邻居数不多于 $S_k$ 个；
- 生成目标节点的embedding：先聚合二跳邻居的特征，生成一跳邻居的embedding，再聚合一跳的embedding，生成目标节点的embedding；
- 将目标节点的embedding输入全连接网络得到目标节点的预测值。

下面是原论文的算法流程：

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="figures/5_graphsage_algorithm.png" width=400> 
    <br>
    <div style="color:orange;
    display: inline-block;
    color: #999;
    padding: 2px;">图5-2. GraphSAGE前向传播的算法流程</div>
</center>


## 5.3 GraphSAGE 代码实现

下面我们介绍一下GraphSAGE的代码实现，使用的是dgl框架。我们用link prediction作为模型的任务来举例。我们先简单的介绍一下链接预测这个任务。许多应用，如社交推荐、项目推荐、知识图谱补全等，都可以表述为链接预测，即预测两个特定节点之间是否存在边。

```python
# 导入相关的库
import dgl
import torch
import torch.nn as nn
import torch.nn.functional as F
import itertools
import numpy as np
import scipy.sparse as sp
```

```python
# 导入Cora数据集
import dgl.data

dataset = dgl.data.CoraGraphDataset()
g = dataset[0]
```

```python
# 准备training set 和 testing set
u, v = g.edges()

eids = np.arange(g.number_of_edges())
eids = np.random.permutation(eids)
test_size = int(len(eids) * 0.1)
train_size = g.number_of_edges() - test_size
test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]
train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]

# 分离负样本
adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))
adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())
neg_u, neg_v = np.where(adj_neg != 0)

neg_eids = np.random.choice(len(neg_u), g.number_of_edges())
test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]
train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]
```

下面我们正式定义一个GraphSAGE模型：

```python
from dgl.nn import SAGEConv

# 构建一个两层的 GraphSAGE 模型
class GraphSAGE(nn.Module):
    def __init__(self, in_feats, h_feats):
        super(GraphSAGE, self).__init__()
        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')
        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')

    def forward(self, g, in_feat):
        h = self.conv1(g, in_feat)
        h = F.relu(h)
        h = self.conv2(g, h)
        return h
```

然后，该模型通过计算两个节点的表示之间的得分来预测边缘存在的概率，通常通过一层MLP或者直接计算点积。

```python
# 构建正样本和负样本的图
train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())
train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())

test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())
test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())
```

构建上面提到的预测函数，如点积和MLP，即 DotPredictor 和 MLPPredictor:

```python
import dgl.function as fn

class DotPredictor(nn.Module):
    def forward(self, g, h):
        with g.local_scope():
            g.ndata['h'] = h
            # 通过点积计算一个新的边的分数
            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))
            # u_dot_v 返回了一个 1-element 的向量，所以需要压平它
            return g.edata['score'][:, 0]

class MLPPredictor(nn.Module):
    def __init__(self, h_feats):
        super().__init__()
        self.W1 = nn.Linear(h_feats * 2, h_feats)
        self.W2 = nn.Linear(h_feats, 1)

    def apply_edges(self, edges):
        """
        Computes a scalar score for each edge of the given graph.

        Parameters
        ----------
        edges :
            Has three members ``src``, ``dst`` and ``data``, each of
            which is a dictionary representing the features of the
            source nodes, the destination nodes, and the edges
            themselves.

        Returns
        -------
        dict
            A dictionary of new edge features.
        """
        h = torch.cat([edges.src['h'], edges.dst['h']], 1)
        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}

    def forward(self, g, h):
        with g.local_scope():
            g.ndata['h'] = h
            g.apply_edges(self.apply_edges)
            return g.edata['score']
```

下面展示整个任务的训练过程

```python
model = GraphSAGE(train_g.ndata['feat'].shape[1], 16)
# You can replace DotPredictor with MLPPredictor.
# pred = MLPPredictor(16)
pred = DotPredictor()

def compute_loss(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score])
    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])
    return F.binary_cross_entropy_with_logits(scores, labels)

def compute_auc(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score]).numpy()
    labels = torch.cat(
        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()
    return roc_auc_score(labels, scores)
```

```python

optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)

# 训练
all_logits = []
for e in range(100):
    # 前向传播
    h = model(train_g, train_g.ndata['feat'])
    pos_score = pred(train_pos_g, h)
    neg_score = pred(train_neg_g, h)
    loss = compute_loss(pos_score, neg_score)

    # 更新参数
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if e % 5 == 0:
        print('In epoch {}, loss: {}'.format(e, loss))

# 计算AUC
from sklearn.metrics import roc_auc_score
with torch.no_grad():
    pos_score = pred(test_pos_g, h)
    neg_score = pred(test_neg_g, h)
    print('AUC', compute_auc(pos_score, neg_score))

```